{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "-e_D8RNjq97s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HjAPRywypL0O"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def fetch_csv(file_id):\n",
        "    url = f'https://drive.google.com/uc?id={file_id}&export=download'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
        "    else:\n",
        "        print(f\"Error fetching the file with ID: {file_id}\")\n",
        "        return None\n",
        "\n",
        "customers_id = '1bu_--mo79VdUG9oin4ybfFGRUSXAe-WE'\n",
        "products_id = '1IKuDizVapw-hyktwfpoAoaGtHtTNHfd0'\n",
        "transactions_id = '1saEqdbBB-vuk2hxoAf4TzDEsykdKlzbF'\n",
        "\n",
        "customers_df = fetch_csv(customers_id)\n",
        "products_df = fetch_csv(products_id)\n",
        "transactions_df = fetch_csv(transactions_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation\n",
        "transactions_df['TransactionDate'] = pd.to_datetime(transactions_df['TransactionDate'])\n",
        "customers_df['SignupDate'] = pd.to_datetime(customers_df['SignupDate'])\n",
        "\n",
        "# Merge datasets\n",
        "merged_data = transactions_df.merge(\n",
        "    customers_df, on='CustomerID', how='left'\n",
        ").merge(\n",
        "    products_df.rename(columns={'Price': 'ProductPrice'}),\n",
        "    on='ProductID', how='left'\n",
        ")\n",
        "\n",
        "# Feature Engineering\n",
        "def create_customer_features(df):\n",
        "    # Profile Features\n",
        "    latest_date = df['TransactionDate'].max()\n",
        "    customer_features = df.groupby('CustomerID').agg(\n",
        "        Region=('Region', 'first'),\n",
        "        Tenure=('SignupDate', lambda x: (latest_date - x.max()).days),\n",
        "        TotalTransactions=('TransactionID', 'nunique'),\n",
        "        TotalSpend=('TotalValue', 'sum'),\n",
        "        AvgSpend=('TotalValue', 'mean'),\n",
        "        FavoriteCategory=('Category', lambda x: x.mode()[0])\n",
        "    ).reset_index()\n",
        "\n",
        "    # Product Interaction Features\n",
        "    category_counts = pd.pivot_table(\n",
        "        df,\n",
        "        index='CustomerID',\n",
        "        columns='Category',\n",
        "        values='TransactionID',\n",
        "        aggfunc='count',\n",
        "        fill_value=0\n",
        "    ).add_prefix('Category_')\n",
        "\n",
        "    return customer_features.merge(category_counts, on='CustomerID')\n",
        "\n",
        "feature_matrix = create_customer_features(merged_data)\n",
        "\n",
        "# Preprocessing\n",
        "encoder = OneHotEncoder()\n",
        "region_encoded = encoder.fit_transform(feature_matrix[['Region']])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "numeric_features = feature_matrix[['Tenure', 'TotalTransactions', 'TotalSpend', 'AvgSpend']]\n",
        "scaled_features = scaler.fit_transform(numeric_features)\n",
        "\n",
        "category_features = feature_matrix.drop(\n",
        "    columns=['CustomerID', 'Region', 'Tenure', 'TotalTransactions', 'TotalSpend', 'AvgSpend', 'FavoriteCategory']\n",
        ").values\n",
        "\n",
        "processed_features = np.hstack([\n",
        "    region_encoded.toarray(),\n",
        "    scaled_features,\n",
        "    category_features\n",
        "])\n",
        "\n",
        "# Similarity Calculation\n",
        "similarity_matrix = cosine_similarity(processed_features)\n",
        "customer_ids = feature_matrix['CustomerID'].values\n",
        "\n",
        "# Generate Recommendations\n",
        "target_customers = [f\"C{str(i+1).zfill(4)}\" for i in range(20)]\n",
        "lookalike_mapping = {}\n",
        "\n",
        "for target in target_customers:\n",
        "    if target not in customer_ids:\n",
        "        lookalike_mapping[target] = []\n",
        "        continue\n",
        "\n",
        "    idx = np.where(customer_ids == target)[0][0]\n",
        "    scores = list(enumerate(similarity_matrix[idx]))\n",
        "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)[1:4]  # Exclude self\n",
        "\n",
        "    lookalike_mapping[target] = [\n",
        "        (customer_ids[i], float(round(score, 4)))\n",
        "        for i, score in sorted_scores\n",
        "    ]\n",
        "\n",
        "# Save to CSV\n",
        "output_data = []\n",
        "for cust_id, matches in lookalike_mapping.items():\n",
        "    output_data.append({\n",
        "        'CustomerID': cust_id,\n",
        "        'Lookalikes': [[match[0], match[1]] for match in matches]\n",
        "    })\n",
        "\n",
        "pd.DataFrame(output_data).to_csv('Lookalike.csv', index=False)"
      ],
      "metadata": {
        "id": "cUVkvLFtriWm"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}